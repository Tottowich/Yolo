{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Tasks Supported by YOLOv8:\n",
    "\n",
    "1. **Detection**:\n",
    "   - Objective: Detect objects in an image or video frame and draw bounding boxes around them.\n",
    "   - Use Case: Object detection in various applications such as surveillance, autonomous vehicles, and robotics.\n",
    "\n",
    "2. **Segmentation**:\n",
    "   - Objective: Segment an image into different regions based on its content and assign labels to each region.\n",
    "   - Use Case: Image segmentation, medical imaging, and understanding object boundaries.\n",
    "\n",
    "3. **Classification**:\n",
    "   - Objective: Classify an image into different categories based on its content.\n",
    "   - Use Case: Image classification in various applications such as object recognition, content filtering, and visual search.\n",
    "\n",
    "4. **Pose/Keypoint Detection**:\n",
    "   - Objective: Detect specific points (keypoints) in an image or video frame for tracking movement or pose estimation.\n",
    "   - Use Case: Human pose estimation, motion tracking, and gesture recognition.\n",
    "\n",
    "Note: YOLOv8 utilizes different architectures (e.g., U-Net, EfficientNet) to perform segmentation, classification, and pose/keypoint detection, ensuring accuracy and speed in these tasks.\n",
    "### Models Supported by YOLOv8:\n",
    "The following models are available in YOLOv8 and are suitable starting points for various tasks:\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "| Model Type   | Pre-trained Weights                            | Task                |\n",
    "|--------------|------------------------------------------------|---------------------|\n",
    "| YOLOv8       | yolov8n.pt, yolov8s.pt, yolov8m.pt, yolov8l.pt, yolov8x.pt       | Detection           |\n",
    "| YOLOv8-seg   | yolov8n-seg.pt, yolov8s-seg.pt, yolov8m-seg.pt, yolov8l-seg.pt, yolov8x-seg.pt   | Instance Segmentation |\n",
    "| YOLOv8-pose  | yolov8n-pose.pt, yolov8s-pose.pt, yolov8m-pose.pt, yolov8l-pose.pt, yolov8x-pose.pt, yolov8x-pose-p6 | Pose/Keypoints      |\n",
    "| YOLOv8-cls   | yolov8n-cls.pt, yolov8s-cls.pt, yolov8m-cls.pt, yolov8l-cls.pt, yolov8x-cls.pt     | Classification      |\n",
    "\n",
    "</div>\n",
    "\n",
    "## Model and project settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model    = 'yolov8n.pt'                                    # Initial weights path. Can aslo be a .yaml defining a model, e.g. yolov8m.yaml\n",
    "task     = 'detect'                                        # 'detect', 'classify', 'segment'\n",
    "project  = 'yolov8/'                                       # Project folder\n",
    "name     = 'digit8n_edges'                                 # Model name\n",
    "exist_ok = True                                            # Overwrite existing project folder\n",
    "seed     = 0                                               # Seed for training\n",
    "resume   = False                                           # Resume training from last.pt from project + name\n",
    "verbose  = False                                           # Print detailed results\n",
    "plots    = False                                           # Plot training results\n",
    "project_dir = os.path.join(os.getcwd(), project, name)     # Project directory\n",
    "device   = '0'                                             # CUDA device, i.e. '0' or '0,1,2,3' or 'cpu'\n",
    "\n",
    "project_settings = {\n",
    "    'name': name,\n",
    "    'project': project,\n",
    "    'exist_ok': exist_ok,\n",
    "    'seed': seed,\n",
    "    'verbose': verbose,\n",
    "    'resume': resume,\n",
    "    'plots': plots,\n",
    "    'device': device,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Datasets\n",
    "These datasets may be used to train or pre-train a model in YOLOv8. The datasets are can also be found [here](https://docs.ultralytics.com/datasets/).\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Detection\n",
    "</summary>\n",
    "\n",
    "| Dataset       | Description                                                      |\n",
    "|---------------|------------------------------------------------------------------|\n",
    "| Argoverse     | 3D tracking and motion forecasting data from urban environments. |\n",
    "| COCO          | Large-scale dataset for object detection, segmentation, and captioning. |\n",
    "| COCO8         | Subset of COCO train and COCO val for quick tests.               |\n",
    "| Global Wheat 2020 | Dataset of wheat head images for object detection and localization. |\n",
    "| Objects365    | High-quality dataset for object detection with 365 categories.   |\n",
    "| SKU-110K      | Dense object detection dataset in retail environments.           |\n",
    "| VisDrone      | Dataset with object detection and multi-object tracking from drone-captured imagery. |\n",
    "| VOC           | Pascal Visual Object Classes dataset for object detection and segmentation. |\n",
    "| xView         | Dataset for object detection in overhead imagery.                |\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Instance Segmentation\n",
    "</summary>\n",
    "\n",
    "| Dataset       | Description                                                      |\n",
    "|---------------|------------------------------------------------------------------|\n",
    "| COCO          | Large-scale dataset for object detection, segmentation, and captioning. |\n",
    "| COCO8-seg     | Subset of COCO with segmentation annotations.                    |\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Pose Estimation\n",
    "</summary>\n",
    "\n",
    "| Dataset       | Description                                                      |\n",
    "|---------------|------------------------------------------------------------------|\n",
    "| COCO          | Large-scale dataset with human pose annotations.                  |\n",
    "| COCO8-pose    | Subset of COCO with human pose annotations.                       |\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Classification\n",
    "</summary>\n",
    "\n",
    "| Dataset       | Description                                                      |\n",
    "|---------------|------------------------------------------------------------------|\n",
    "| Caltech 101   | Dataset with images of 101 object categories for classification.  |\n",
    "| Caltech 256   | Extended version of Caltech 101 with 256 categories.             |\n",
    "| CIFAR-10      | Dataset of color images in 10 classes.                            |\n",
    "| CIFAR-100     | Extended version of CIFAR-10 with 100 categories.                 |\n",
    "| Fashion-MNIST | Dataset with grayscale images of fashion categories.              |\n",
    "| ImageNet      | Large-scale dataset for object detection and classification.      |\n",
    "| ImageNet-10   | Subset of ImageNet with 10 categories.                            |\n",
    "| Imagewoof     | Challenging subset of ImageNet with 10 dog breed categories.      |\n",
    "| Imagenette    | Smaller subset of ImageNet with 10 easily distinguishable classes.|\n",
    "| MNIST         | Dataset of grayscale images of handwritten digits.                |\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Multi-Object Tracking\n",
    "</summary>\n",
    "\n",
    "| Dataset       | Description                                                      |\n",
    "|---------------|------------------------------------------------------------------|\n",
    "| Argoverse     | 3D tracking and motion forecasting data from urban environments. |\n",
    "| VisDrone      | Dataset with object detection and multi-object tracking from drone-captured imagery. |\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Constructing a YOLO Custom Dataset\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "Step-by-step\n",
    "</summary>\n",
    "\n",
    "1. Collect and Organize Your Data:\n",
    "   - Gather images relevant to the object(s) you want to detect.\n",
    "   - Create separate directories for images and annotation files.\n",
    "\n",
    "2. Annotate the Images:\n",
    "   - Use an annotation tool to mark the bounding boxes around objects of interest.\n",
    "   - Save annotations in a YOLO-compatible format.\n",
    "   - Text files with one row per object instance: ```<class_id> <x_center> <y_center> <width> <height>```\n",
    "   - This can be done using various tools such as [Roboflow](https://roboflow.com/) **online** or [YoloLabel](https://github.com/developer0hye/Yolo_Label) **offline**.\n",
    "\n",
    "3. Split the Dataset:\n",
    "   - Divide your dataset into training, validation, and testing sets.\n",
    "   - Ensure each set has a representative distribution of classes and object instances.\n",
    "\n",
    "4. Generate data.yaml file:\n",
    "   - Contains information about the dataset and paths to the training, validation, and testing sets.\n",
    "   - Class names are listed in the order of their IDs in ```names```.\n",
    "   - Number of classes is specified in the ```nc``` field.\n",
    "   - Example:\n",
    "   ```yaml\n",
    "      names:\n",
    "      - '0'\n",
    "      - '1'\n",
    "      - '2'\n",
    "      - '3'\n",
    "      - '4'\n",
    "      - '5'\n",
    "      - '6'\n",
    "      - '7'\n",
    "      - '8'\n",
    "      - '9'\n",
    "      nc: 10\n",
    "      path: /path/to/dataset/ # Path to dataset directory.\n",
    "      test: test              # Relative to path above.\n",
    "      train: train            \n",
    "      val: val               \n",
    "   ```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data settings\n",
    "```python\n",
    "data_path  = '/home/thjo/Datasets/BolidenDigits/data.yaml' # Path to data.yaml\n",
    "imgsz      = 416                                           # Image size.\n",
    "batch_size = -1                                           # Batch size, '-1' uses the largest batch size that fits on the GPS(s)     \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = '/home/thjo/Datasets/BolidenDigits/data.yaml'      # Path to data.yaml\n",
    "imgsz      = 416                                           # Image size.\n",
    "batch      = 64                                            # Batch size, '-1' uses the largest batch size that fits on the GPU(s)\n",
    "fraction   = 1.0                                           # Fraction of dataset to use for training (0-1) Useful for debugging to validate code and convergence\n",
    "\n",
    "data_settings ={\n",
    "    'data': data,\n",
    "    'imgsz': imgsz,\n",
    "    'batch': batch,\n",
    "    'fraction': fraction\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training settings\n",
    "The following are a subset of the available training settings. For a complete list, see [here](https://docs.ultralytics.com/modes/train/#arguments).\n",
    "\n",
    "<details>\n",
    "<summary>lr0 - Initial Learning Rate</summary>\n",
    "The lr0 parameter represents the initial learning rate of the model. It determines the step size at the beginning of training, influencing how quickly the model learns from the data. A higher learning rate can lead to faster convergence, but it may also cause instability or overshooting. Conversely, a lower learning rate may result in slower convergence but can lead to more accurate and stable training.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>val - Validation during Training</summary>\n",
    "The val parameter is a boolean flag indicating whether to validate the model on the validation set during training. Validation allows monitoring the model's performance on unseen data and helps in detecting overfitting or underfitting. By evaluating the model's performance on the validation set, you can make informed decisions regarding model selection and hyperparameter tuning.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>epochs - Number of Epochs</summary>\n",
    "The epochs parameter determines the total number of times the model will iterate over the entire training dataset. An epoch represents a complete pass through the entire training data, and each epoch updates the model's parameters based on the optimization algorithm used. Choosing an appropriate number of epochs is important to balance training time and model convergence. Too few epochs may result in an undertrained model, while too many epochs may lead to overfitting.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>patience - Early Stopping</summary>\n",
    "The patience parameter refers to the number of epochs to wait before stopping training if there is no improvement in the validation metric. It enables early stopping, a technique used to prevent overfitting and improve efficiency. If the model's performance on the validation set does not improve for a specified number of epochs (defined by patience), training is stopped early to avoid wasting computational resources on a non-improving model.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>save_period - Model Saving</summary>\n",
    "The save_period parameter determines how frequently the model is saved during training. Setting a positive value for save_period means that the model will be saved every n epochs. This allows you to have checkpoints of the model at regular intervals during training. Alternatively, setting save_period to -1 disables automatic saving of the model. Manually saving the model can be done at any desired point in the code.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>device - Hardware Device</summary>\n",
    "The device parameter specifies the hardware device to use for training the model. If you have a compatible GPU, you can specify the CUDA device ID (e.g., '0') to utilize GPU acceleration, which can significantly speed up the training process. Alternatively, you can set device to 'cpu' for CPU training. Choosing the appropriate device depends on the availability of hardware resources and the size of the dataset.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>cache - Image Caching</summary>\n",
    "The cache parameter determines whether to cache images for faster training. Caching preprocessed images in memory can improve the training speed by reducing disk I/O and preprocessing overhead. However, caching can consume a significant amount of memory, so it is advisable to consider the available memory resources and the size of the dataset before enabling this option.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>workers - CPU Workers</summary>\n",
    "The workers parameter specifies the number of CPU workers to use for data loading during training. Increasing the number of workers enables parallel data loading, which can speed up the training process, especially when there are bottlenecks in data loading and preprocessing. However, the optimal number of workers depends on the available CPU resources and the complexity of data loading operations.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>optimizer - Optimization Algorithm</summary>\n",
    "The optimizer parameter determines the algorithm used to update the model's parameters during training. Options include SGD (Stochastic Gradient Descent), Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, or 'auto' to automatically select an optimizer based on the model architecture and problem. Each optimizer has its own set of hyperparameters that control the learning process. Choosing an appropriate optimizer and its hyperparameters is crucial for achieving good training performance.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>cos_lr - Cosine Learning Rate Scheduler</summary>\n",
    "The cos_lr parameter indicates whether to use a cosine learning rate scheduler. The cosine learning rate scheduler gradually reduces the learning rate during training. \n",
    "This technique is based on the cosine function and can help the model converge to a better optima. Using a cosine learning rate scheduler can potentially improve the model's performance and generalization.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr0          = 0.005                                      # Initial learning rate\n",
    "val          = False                                      # Validate on validation set during training\n",
    "epochs       = 100                                        # Number of epochs\n",
    "patience     = 25                                         # Stop training after this many epochs without improvement (early stopping)\n",
    "save_period  = -1                                         # Save model every n epochs, -1 to disable\n",
    "device       = '0'                                        # Device (cuda device id)\n",
    "cache        = False                                      # Cache images for faster training\n",
    "workers      = 8                                          # Number of CPU workers\n",
    "optimizer    = 'auto'                                     # Optimizer (SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto)\n",
    "cos_lr       = True                                       # Use cosine learning rate scheduler\n",
    "lrf          = 0.0001                                     # Final learning rate (for cosine scheduler)\n",
    "momentum     = 0.937                                      # SGD momentum/Adam beta1\n",
    "weight_decay = 0.0005                                     # optimizer weight decay\n",
    "warmup_epochs= 3                                          # Warmup epochs (fractions ok)\n",
    "warmup_momentum = 0.8                                     # Warmup initial momentum\n",
    "\n",
    "training_settings = {\n",
    "    'lr0': lr0,\n",
    "    'val': val,\n",
    "    'epochs': epochs,\n",
    "    'patience': patience,\n",
    "    'save_period': save_period,\n",
    "    'device': device,\n",
    "    'cache': cache,\n",
    "    'workers': workers,\n",
    "    'optimizer': optimizer,\n",
    "    'cos_lr': cos_lr,\n",
    "    'lrf': lrf,\n",
    "    'momentum': momentum,\n",
    "    'weight_decay': weight_decay,\n",
    "    'warmup_epochs': warmup_epochs,\n",
    "    'warmup_momentum': warmup_momentum\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations\n",
    "YoloV8 supports a wide range of augmentations. For a complete list, see [here](https://docs.ultralytics.com/usage/cfg/#augmentation).\n",
    "\n",
    "<div align=center>\n",
    "\n",
    "\n",
    "| Parameter    | Value  | Description                                 |\n",
    "|--------------|--------|---------------------------------------------|\n",
    "| hsv_h        | 0.015  | Image HSV-Hue augmentation (fraction)        |\n",
    "| hsv_s        | 0.7    | Image HSV-Saturation augmentation (fraction) |\n",
    "| hsv_v        | 0.4    | Image HSV-Value augmentation (fraction)      |\n",
    "| degrees      | 0.0    | Image rotation (+/- deg)                     |\n",
    "| translate    | 0.1    | Image translation (+/- fraction)             |\n",
    "| scale        | 0.5    | Image scale (+/- gain)                       |\n",
    "| shear        | 0.0    | Image shear (+/- deg)                        |\n",
    "| perspective  | 0.0    | Image perspective (+/- fraction)             |\n",
    "| flipud       | 0.0    | Image flip up-down (probability)             |\n",
    "| fliplr       | 0.5    | Image flip left-right (probability)          |\n",
    "| mosaic       | 1.0    | Image mosaic (probability)                   |\n",
    "| mixup        | 0.0    | Image mixup (probability)                    |\n",
    "| copy_paste   | 0.0    | Segment copy-paste (probability)             |\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "### Standard Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = {\n",
    "    'hsv_h': 0.015,\n",
    "    'hsv_s': 0.7,\n",
    "    'hsv_v': 0.4,\n",
    "    'degrees': 10.0,\n",
    "    'translate': 0.1,\n",
    "    'scale': 0.5,\n",
    "    'shear': 0.0,\n",
    "    'perspective': 0.0,\n",
    "    'flipud': 0.0,\n",
    "    'fliplr': 0.0,\n",
    "    'mosaic': 1.0,\n",
    "    'mixup': 0.0,\n",
    "    'copy_paste': 0.0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    **project_settings,\n",
    "    **data_settings,\n",
    "    **training_settings,\n",
    "    **augmentations\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.116 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.114 🚀 Python-3.9.16 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 7973MiB)\n",
      "\u001b[34m\u001b[1myolo/engine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/home/thjo/Datasets/BolidenDigits/data.yaml, epochs=100, patience=25, batch=64, imgsz=416, save=True, save_period=-1, cache=False, device=0, workers=8, project=yolov8/, name=digit8n_edges, exist_ok=True, pretrained=False, optimizer=auto, verbose=False, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=True, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=False, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=False, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.005, lrf=0.0001, momentum=0.937, weight_decay=0.0005, warmup_epochs=3, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=10.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=yolov8/digit8n_edges\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753262  ultralytics.nn.modules.head.Detect           [10, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3012798 parameters, 3012782 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/thjo/Datasets/BolidenDigits/train.cache... 1703 images, 67 backgrounds, 0 corrupt: 100%|██████████| 1703/1703 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/thjo/Datasets/BolidenDigits/val.cache... 200 images, 9 backgrounds, 0 corrupt: 100%|██████████| 200/200 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 416 train, 416 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1myolov8/digit8n_edges\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      1/100      3.65G      1.864      3.856      1.691        298        416: 100%|██████████| 27/27 [00:06<00:00,  3.92it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      2/100      3.66G       1.34      2.272      1.272        319        416: 100%|██████████| 27/27 [00:04<00:00,  6.62it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      3/100       3.7G      1.174      1.572      1.198        310        416: 100%|██████████| 27/27 [00:03<00:00,  6.94it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      4/100      3.64G      1.135      1.338      1.164        305        416: 100%|██████████| 27/27 [00:04<00:00,  6.66it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      5/100      3.65G       1.09      1.207      1.149        247        416: 100%|██████████| 27/27 [00:04<00:00,  6.59it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      6/100       3.7G      1.074      1.149      1.135        292        416: 100%|██████████| 27/27 [00:04<00:00,  5.89it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      7/100      3.64G      1.016      1.065      1.118        272        416: 100%|██████████| 27/27 [00:04<00:00,  5.96it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      8/100      3.64G     0.9814      1.025      1.106        266        416: 100%|██████████| 27/27 [00:04<00:00,  5.87it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      9/100      3.64G     0.9946     0.9888      1.106        265        416: 100%|██████████| 27/27 [00:04<00:00,  5.83it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     10/100      3.64G      0.978     0.9832      1.099        233        416: 100%|██████████| 27/27 [00:04<00:00,  5.99it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     11/100      3.65G     0.9515     0.9618      1.096        283        416: 100%|██████████| 27/27 [00:04<00:00,  5.99it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     12/100       3.7G     0.9573     0.9319      1.088        259        416: 100%|██████████| 27/27 [00:04<00:00,  6.31it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     13/100      3.64G     0.9139     0.9074      1.076        305        416: 100%|██████████| 27/27 [00:04<00:00,  6.17it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     14/100       3.7G     0.8947     0.8983      1.069        281        416: 100%|██████████| 27/27 [00:04<00:00,  5.83it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     15/100       3.7G      0.926     0.8757       1.08        303        416: 100%|██████████| 27/27 [00:04<00:00,  5.99it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     16/100      3.64G      0.874     0.8496      1.063        290        416: 100%|██████████| 27/27 [00:04<00:00,  5.88it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     17/100       3.7G     0.8781     0.8288      1.058        283        416: 100%|██████████| 27/27 [00:04<00:00,  6.18it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     18/100      3.65G     0.8735     0.8154      1.061        343        416: 100%|██████████| 27/27 [00:04<00:00,  6.01it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     19/100      3.67G      0.868     0.8243      1.059        314        416: 100%|██████████| 27/27 [00:04<00:00,  6.20it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     20/100      3.69G     0.8558     0.8018      1.053        260        416: 100%|██████████| 27/27 [00:04<00:00,  6.09it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     21/100       3.7G     0.8136     0.7634      1.034        273        416: 100%|██████████| 27/27 [00:04<00:00,  6.23it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     22/100      3.69G     0.8515      0.768      1.055        270        416: 100%|██████████| 27/27 [00:04<00:00,  6.25it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     23/100      3.64G     0.8421     0.7645      1.046        263        416: 100%|██████████| 27/27 [00:04<00:00,  5.87it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     24/100       3.7G     0.8453      0.778      1.054        305        416: 100%|██████████| 27/27 [00:04<00:00,  6.08it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     25/100       3.7G     0.8561     0.7534      1.045        270        416: 100%|██████████| 27/27 [00:04<00:00,  6.02it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     26/100      3.69G     0.8039     0.7307      1.026        315        416: 100%|██████████| 27/27 [00:04<00:00,  6.50it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     27/100      3.64G     0.8017     0.7343       1.03        287        416: 100%|██████████| 27/27 [00:04<00:00,  6.05it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     28/100      3.69G     0.8107     0.7324      1.027        276        416: 100%|██████████| 27/27 [00:04<00:00,  6.54it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     29/100       3.7G     0.7839     0.6969      1.021        291        416: 100%|██████████| 27/27 [00:04<00:00,  6.57it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     30/100      3.64G     0.7891     0.7128      1.026        294        416: 100%|██████████| 27/27 [00:04<00:00,  6.66it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     31/100      3.64G     0.7713     0.6795      1.018        314        416: 100%|██████████| 27/27 [00:15<00:00,  1.80it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     32/100      3.64G     0.7818     0.6927      1.016        288        416: 100%|██████████| 27/27 [00:15<00:00,  1.80it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     33/100       3.7G     0.7642     0.6738      1.016        293        416: 100%|██████████| 27/27 [00:15<00:00,  1.70it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     34/100      3.64G     0.7755     0.6727      1.016        315        416: 100%|██████████| 27/27 [00:04<00:00,  6.16it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     35/100      3.69G     0.7723     0.6733      1.013        280        416: 100%|██████████| 27/27 [00:04<00:00,  6.14it/s]\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "     36/100      3.69G     0.7777     0.6676      1.015        486        416:  93%|█████████▎| 25/27 [00:04<00:00,  5.95it/s]"
     ]
    }
   ],
   "source": [
    "model = YOLO(model=model, task=task)\n",
    "model.train(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transferred 595/595 items from pretrained weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ultralytics.yolo.engine.model.YOLO at 0x7f9bd8f5cc10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load(os.path.join(project_dir, 'weights', 'best.pt'))  # load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.114 🚀 Python-3.9.16 torch-2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Ti, 7973MiB)\n",
      "Model summary (fused): 268 layers, 68133198 parameters, 0 gradients, 257.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/thjo/Datasets/BolidenDigits/val.cache... 77 images, 0 backgrounds, 0 corrupt: 100%|██████████| 77/77 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:02<00:00,  4.71it/s]\n",
      "                   all         77        269      0.952      0.898      0.954      0.783\n",
      "Speed: 0.2ms preprocess, 22.4ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1myolov8/digit8x_extended\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "val_results = model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39mexport(\u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39monnx\u001b[39m\u001b[39m'\u001b[39m, dynamic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, opset\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.export(format='onnx', dynamic=True, simplify=True, opset=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Results saved to \u001b[1myolov8/digit8x_extended\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.predict('/home/thjo/Datasets/BolidenDigits/test/', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ultralytics.yolo.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.yolo.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
       " orig_img: array([[[122, 106, 124],\n",
       "         [122, 106, 124],\n",
       "         [123, 107, 125],\n",
       "         ...,\n",
       "         [125, 109, 127],\n",
       "         [125, 109, 127],\n",
       "         [125, 109, 127]],\n",
       " \n",
       "        [[122, 106, 124],\n",
       "         [122, 106, 124],\n",
       "         [123, 107, 125],\n",
       "         ...,\n",
       "         [125, 109, 127],\n",
       "         [125, 109, 127],\n",
       "         [125, 109, 127]],\n",
       " \n",
       "        [[122, 106, 124],\n",
       "         [122, 106, 124],\n",
       "         [123, 107, 125],\n",
       "         ...,\n",
       "         [125, 109, 127],\n",
       "         [125, 109, 127],\n",
       "         [125, 109, 127]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[113,  97, 115],\n",
       "         [113,  97, 115],\n",
       "         [113,  97, 115],\n",
       "         ...,\n",
       "         [114,  98, 116],\n",
       "         [114,  98, 116],\n",
       "         [114,  98, 116]],\n",
       " \n",
       "        [[113,  97, 115],\n",
       "         [113,  97, 115],\n",
       "         [113,  97, 115],\n",
       "         ...,\n",
       "         [114,  98, 116],\n",
       "         [114,  98, 116],\n",
       "         [114,  98, 116]],\n",
       " \n",
       "        [[113,  97, 115],\n",
       "         [113,  97, 115],\n",
       "         [113,  97, 115],\n",
       "         ...,\n",
       "         [114,  98, 116],\n",
       "         [114,  98, 116],\n",
       "         [114,  98, 116]]], dtype=uint8)\n",
       " orig_shape: (207, 481)\n",
       " path: '/home/thjo/Datasets/BolidenDigits/manualDigits/test/20230611_085605_10_neurosal.jpg'\n",
       " probs: None\n",
       " speed: {'preprocess': 17.200231552124023, 'inference': 78.99594306945801, 'postprocess': 1.16729736328125},\n",
       " ultralytics.yolo.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.yolo.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
       " orig_img: array([[[60, 50, 62],\n",
       "         [60, 50, 62],\n",
       "         [60, 50, 62],\n",
       "         ...,\n",
       "         [64, 54, 66],\n",
       "         [63, 53, 65],\n",
       "         [63, 53, 65]],\n",
       " \n",
       "        [[60, 50, 62],\n",
       "         [60, 50, 62],\n",
       "         [60, 50, 62],\n",
       "         ...,\n",
       "         [64, 54, 66],\n",
       "         [63, 53, 65],\n",
       "         [62, 52, 64]],\n",
       " \n",
       "        [[60, 50, 62],\n",
       "         [60, 50, 62],\n",
       "         [60, 50, 62],\n",
       "         ...,\n",
       "         [64, 54, 66],\n",
       "         [63, 53, 65],\n",
       "         [63, 53, 65]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[62, 52, 64],\n",
       "         [62, 52, 64],\n",
       "         [62, 52, 64],\n",
       "         ...,\n",
       "         [63, 53, 65],\n",
       "         [63, 53, 65],\n",
       "         [63, 53, 65]],\n",
       " \n",
       "        [[61, 51, 63],\n",
       "         [62, 52, 64],\n",
       "         [62, 52, 64],\n",
       "         ...,\n",
       "         [63, 53, 65],\n",
       "         [63, 53, 65],\n",
       "         [63, 53, 65]],\n",
       " \n",
       "        [[61, 51, 63],\n",
       "         [61, 51, 63],\n",
       "         [62, 52, 64],\n",
       "         ...,\n",
       "         [63, 53, 65],\n",
       "         [63, 53, 65],\n",
       "         [63, 53, 65]]], dtype=uint8)\n",
       " orig_shape: (200, 403)\n",
       " path: '/home/thjo/Datasets/BolidenDigits/manualDigits/test/20230611_085605_1_neurosal.jpg'\n",
       " probs: None\n",
       " speed: {'preprocess': 0.6186962127685547, 'inference': 11.533737182617188, 'postprocess': 1.0886192321777344},\n",
       " ultralytics.yolo.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.yolo.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
       " orig_img: array([[[ 7,  6,  8],\n",
       "         [ 7,  6,  8],\n",
       "         [ 7,  6,  8],\n",
       "         ...,\n",
       "         [ 1,  3,  3],\n",
       "         [ 1,  3,  3],\n",
       "         [ 1,  3,  3]],\n",
       " \n",
       "        [[ 7,  6,  8],\n",
       "         [ 7,  6,  8],\n",
       "         [ 7,  6,  8],\n",
       "         ...,\n",
       "         [ 1,  3,  3],\n",
       "         [ 1,  3,  3],\n",
       "         [ 1,  3,  3]],\n",
       " \n",
       "        [[ 7,  6,  8],\n",
       "         [ 7,  6,  8],\n",
       "         [ 7,  6,  8],\n",
       "         ...,\n",
       "         [ 1,  3,  3],\n",
       "         [ 1,  3,  3],\n",
       "         [ 1,  3,  3]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[19, 17, 17],\n",
       "         [19, 17, 17],\n",
       "         [19, 17, 17],\n",
       "         ...,\n",
       "         [ 8,  8,  8],\n",
       "         [ 8,  8,  8],\n",
       "         [ 8,  8,  8]],\n",
       " \n",
       "        [[19, 17, 17],\n",
       "         [19, 17, 17],\n",
       "         [19, 17, 17],\n",
       "         ...,\n",
       "         [ 8,  8,  8],\n",
       "         [ 8,  8,  8],\n",
       "         [ 8,  8,  8]],\n",
       " \n",
       "        [[19, 17, 17],\n",
       "         [19, 17, 17],\n",
       "         [19, 17, 17],\n",
       "         ...,\n",
       "         [ 8,  8,  8],\n",
       "         [ 8,  8,  8],\n",
       "         [ 8,  8,  8]]], dtype=uint8)\n",
       " orig_shape: (1920, 1440)\n",
       " path: '/home/thjo/Datasets/BolidenDigits/manualDigits/test/20230611_085919_120_neurosal.jpg'\n",
       " probs: None\n",
       " speed: {'preprocess': 1.3096332550048828, 'inference': 18.57757568359375, 'postprocess': 1.2052059173583984},\n",
       " ultralytics.yolo.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.yolo.engine.results.Boxes object\n",
       " keypoints: None\n",
       " keys: ['boxes']\n",
       " masks: None\n",
       " names: {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n",
       " orig_img: array([[[21, 18, 20],\n",
       "         [23, 20, 22],\n",
       "         [25, 23, 23],\n",
       "         ...,\n",
       "         [21, 19, 19],\n",
       "         [22, 20, 20],\n",
       "         [22, 20, 20]],\n",
       " \n",
       "        [[22, 19, 21],\n",
       "         [23, 20, 22],\n",
       "         [25, 23, 23],\n",
       "         ...,\n",
       "         [19, 17, 17],\n",
       "         [20, 18, 18],\n",
       "         [21, 19, 19]],\n",
       " \n",
       "        [[22, 20, 20],\n",
       "         [23, 21, 21],\n",
       "         [25, 23, 23],\n",
       "         ...,\n",
       "         [19, 17, 17],\n",
       "         [19, 17, 17],\n",
       "         [20, 18, 18]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[53, 47, 52],\n",
       "         [53, 47, 52],\n",
       "         [53, 47, 52],\n",
       "         ...,\n",
       "         [23, 21, 21],\n",
       "         [23, 21, 21],\n",
       "         [23, 21, 21]],\n",
       " \n",
       "        [[53, 47, 52],\n",
       "         [53, 47, 52],\n",
       "         [53, 47, 52],\n",
       "         ...,\n",
       "         [23, 21, 21],\n",
       "         [23, 21, 21],\n",
       "         [23, 21, 21]],\n",
       " \n",
       "        [[53, 47, 52],\n",
       "         [53, 47, 52],\n",
       "         [53, 47, 52],\n",
       "         ...,\n",
       "         [23, 21, 21],\n",
       "         [23, 21, 21],\n",
       "         [23, 21, 21]]], dtype=uint8)\n",
       " orig_shape: (220, 363)\n",
       " path: '/home/thjo/Datasets/BolidenDigits/manualDigits/test/20230611_085919_137_neurosal.jpg'\n",
       " probs: None\n",
       " speed: {'preprocess': 2.978086471557617, 'inference': 28.278350830078125, 'postprocess': 2.0399093627929688}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.predict('/home/thjo/Datasets/BolidenDigits/manualDigits/test/', show=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bolidenV8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
