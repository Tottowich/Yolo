{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from roboflow import Roboflow\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!py train.py --img 1280 --batch 8 --rect --epochs 60 --data \"Xr-Synthesizer-11/data.yaml\" --weights \"yolov5x6.pt\" --cache \"disk\" --hyp \".\\evolve\\evolve\\Moist-Wind-evolve\\hyp_evolve.yaml\" --save-period 5 --workers 6 --name \"Reborned-Moist-Wind\" --resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!py train.py --img 1280 --batch 6 --rect --epochs 300 --data \"Xr-Synthesizer-11/data.yaml\" --weights '' --cache \"disk\" --hyp \"data/hyps/hyp.scratch-high.yaml\" --save-period 40 --workers 6 --name \"Scratch\" --resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Downloading Dataset Version Zip in Xr-Synthesizer-11 to yolov5pytorch: 100% [163429785 / 163429785] bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Dataset Version Zip to Xr-Synthesizer-11 in yolov5pytorch:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4214/4214 [00:01<00:00, 2839.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"AawnMSY19Y3YNcmMjZWX\")\n",
    "project = rf.workspace(\"lidarimages-9xnln\").project(\"xr-synthesizer\")\n",
    "dataset = project.version(12).download(\"yolov5\") # Box rot, img mask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Downloading Dataset Version Zip in Xr-Synthesizer-10 to yolov5pytorch: 100% [104005246 / 104005246] bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Dataset Version Zip to Xr-Synthesizer-10 in yolov5pytorch:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2898/2898 [00:01<00:00, 2848.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"AawnMSY19Y3YNcmMjZWX\")\n",
    "project = rf.workspace(\"lidarimages-9xnln\").project(\"xr-synthesizer\")\n",
    "dataset = project.version(10).download(\"yolov5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roboflow import Roboflow\n",
    "#Added from outside sarah and lyftet but mosaic\n",
    "rf = Roboflow(api_key=\"AawnMSY19Y3YNcmMjZWX\")\n",
    "project = rf.workspace(\"lidarimages-9xnln\").project(\"xr-synthesizer\")\n",
    "dataset = project.version(5).download(\"yolov5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"AawnMSY19Y3YNcmMjZWX\")\n",
    "project = rf.workspace(\"lidarimages-9xnln\").project(\"xr-synthesizer\")\n",
    "dataset = project.version(4).download(\"yolov5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cuda memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.9 train.py --img 1280 --batch 24 --epochs 10 --data \"Xr-Synthesizer-4/data.yaml\" --weights yolov5m6.pt --cache --hyp \"data/hyps/hyp.VOC.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.9 detect.py --weights runs/train/exp9/weights/best.pt --source \"Xr-Synthesizer-4/test/images/image_1_jpg.rf.e0ba606a9c49c7ae965ffc5f415ffb78.jpg\" --img 1280 --conf 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.9 val.py --weights runs/train/exp5/weights/last.pt --data \"Xr-Synthesizer-4/data.yaml\" --conf 0.3 --img 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3.9 detect.py --weights yolov5x6.pt --source \"Xr-Synthesizer-4/test/images/image_1_jpg.rf.e0ba606a9c49c7ae965ffc5f415ffb78.jpg\" --img 1280 --conf 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidarImages/4_people_standard_room/image_2.jpg,Xr-Synthesizer-4/test/images/image_7_jpg.rf.dc94710b4d559bc61e3baf53a2a309c7.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random((1,64,1024,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random((1,64,1024,3))\n",
    "plt.imshow(X=a[0])\n",
    "plt.show()\n",
    "a = cv2.resize(a[0],(1280,640),interpolation=cv2.INTER_LINEAR)\n",
    "#a = np.stack(a, 0)\n",
    "print(a.shape)\n",
    "plt.imshow(X=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARG:\n",
    "    weights = \"runs/train/Moist-Wind/weights/best.pt\"\n",
    "    imgsz = 1280\n",
    "    data = \"Xr-Synthesizer-6/data.yaml\"\n",
    "    conf_thres = 0.25\n",
    "    iou_thres = 0.45\n",
    "    line_thickness = 3\n",
    "    hide_labels = False\n",
    "    hide_conf = False\n",
    "    half = False\n",
    "    dnn = False\n",
    "    device = 0\n",
    "    auto = True\n",
    "    augment = False\n",
    "    classes = None\n",
    "    OU_ip = \"192.168.200.78\"\n",
    "    udp_port = 7502\n",
    "    tcp_port = 7503\n",
    "    agnostic_nms = False\n",
    "    max_det = 1000\n",
    "    disp_pred = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 2022-07-07 09:12:44,269 - torch_utils - YOLOv5 ðŸš€ v6.1-255-ge733f9f Python-3.9.13 torch-1.10.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3090, 24235MiB)\n",
      "\n",
      "YOLOv5 ðŸš€ v6.1-255-ge733f9f Python-3.9.13 torch-1.10.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3090, 24235MiB)\n",
      "\n",
      "INFO - 2022-07-07 09:12:44,288 - torch_utils - YOLOv5 ðŸš€ v6.1-255-ge733f9f Python-3.9.13 torch-1.10.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3090, 24235MiB)\n",
      "\n",
      "YOLOv5 ðŸš€ v6.1-255-ge733f9f Python-3.9.13 torch-1.10.0+cu113 CUDA:0 (NVIDIA GeForce RTX 3090, 24235MiB)\n",
      "\n",
      "INFO - 2022-07-07 09:12:53,089 - yolo - Fusing layers... \n",
      "Fusing layers... \n",
      "INFO - 2022-07-07 09:12:54,087 - torch_utils - Model summary: 574 layers, 139970872 parameters, 0 gradients, 207.9 GFLOPs\n",
      "Model summary: 574 layers, 139970872 parameters, 0 gradients, 207.9 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "args = ARG()\n",
    "device = select_device(device=args.device)\n",
    "model, stride, names, pt, device,live = initialize_network(args,device)\n",
    "logger = create_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Xr-Synthesizer-4/train/images/image_0_jpg.rf.54d014cb9dce43508282d04d354fca4d.jpg\"\n",
    "\"../lidarImages/4_people_standard_room/image_8.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img0.shape: (640, 1280, 3)\n",
      "img0 type: <class 'numpy.ndarray'>\n",
      "[tensor([[8.88385e+02, 1.16502e+02, 9.91285e+02, 5.66211e+02, 8.21352e-01, 0.00000e+00],\n",
      "        [1.23124e+02, 7.24109e+01, 2.58732e+02, 5.50111e+02, 8.00093e-01, 0.00000e+00],\n",
      "        [5.02442e+02, 1.27744e+02, 5.88631e+02, 4.92214e+02, 7.85585e-01, 0.00000e+00],\n",
      "        [6.98742e+02, 1.56846e+02, 7.72501e+02, 4.68066e+02, 7.45777e-01, 0.00000e+00]], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "img0 = cv2.imread(\"Xr-Synthesizer-6/test/images/image_20_jpg.rf.dea87199a35ff6d3cebac8901951ea57.jpg\").astype(np.float32)\n",
    "print(f\"img0.shape: {img0.shape}\")\n",
    "print(f\"img0 type: {type(img0)}\")\n",
    "\n",
    "img0 /= 255.0\n",
    "img0, img = live.prep(img0)\n",
    "if len(img0.shape) == 3:\n",
    "    img0 = img0[None]\n",
    "img = torch.from_numpy(img).to(device)\n",
    "img = img.half() if model.fp16 else img.float()\n",
    "pred = model(img,augment=args.augment,visualize=False)\n",
    "pred = non_max_suppression(pred, args.conf_thres, args.iou_thres, args.classes, args.agnostic_nms, max_det=args.max_det)\n",
    "s = type(\"Scan\",(object,),{\"h\":64,\"w\":1024,\"d\":3})\n",
    "#obj= proj_vec(pred,img[0],s)\n",
    "#plt.scatter(obj[:,0],obj[:,1])\n",
    "print(pred)\n",
    "#visualize_yolo_test(pred,img,args,logger=logger,names= [\"adult\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/linuxjohansson/Code/XrSynth/Yolo/train_custom.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/linuxjohansson/Code/XrSynth/Yolo/train_custom.ipynb#ch0000026?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/linuxjohansson/Code/XrSynth/Yolo/train_custom.ipynb#ch0000026?line=1'>2</a>\u001b[0m np\u001b[39m.\u001b[39;49mpi()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = type(\"Scan\",(object,),{\"h\":64,\"w\":1024,\"d\":3})\n",
    "def visualize_yolo_test(pred,img,args,scan=s,logger=None,names=None):\n",
    "    detections = 0\n",
    "    obj_dim, heights = proj_alt(pred,img0,scan) \n",
    "    for i,det in enumerate(pred):\n",
    "        detections += 1\n",
    "        img0 = np.ascontiguousarray(copy(img).squeeze().permute(1,2,0).cpu().numpy())\n",
    "        annotator = Annotator(img0, line_width=args.line_thickness, example=str(names))\n",
    "        if len(det):\n",
    "            #print(img.shape[2:],img.squeeze().permute(1,2,0).shape)\n",
    "            det[:,:4] = scale_coords(img.shape[2:], det[:,:4], img0.shape).round()\n",
    "            # if args.disp_pred and logger is not None:\n",
    "            #     for c in det[:, -1].unique():\n",
    "            #         n = (det[:, -1] == c).sum()  # detections per class\n",
    "            #         s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "            # logger.info(f\"{names[int(c)]} detections: {s}\")\n",
    "            i = 0\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                c = int(cls)  # integer class\n",
    "                label = None if args.hide_labels else (names[c] if args.hide_conf else f'{names[c]} {conf:.2f} {heights[i][0]:.2f} {heights[i][1]:.2f}')\n",
    "                i += 1\n",
    "                annotator.box_label(xyxy, label, color=colors(c, True),txt_color=(0,0,0))\n",
    "            img0 = annotator.result()\n",
    "            logger.info(f\"Det: {det}\")\n",
    "            img0 = cv2.cvtColor(img0,cv2.COLOR_RGB2BGR)\n",
    "            #print(f\"Post viz Average img: {img0.mean()}\")\n",
    "            plt.imshow(img0)\n",
    "            plt.show()\n",
    "            #cv2.waitKey(1)\n",
    "        else:\n",
    "            #print(img.shape)\n",
    "            img0  = annotator.result()\n",
    "            img0 = cv2.cvtColor(img0,cv2.COLOR_RGB2BGR)\n",
    "            plt.imshow(img0)\n",
    "            #print(f\"Post viz Average img: {img.mean()}\")\n",
    "            plt.show()\n",
    "            # cv2.waitKey(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_alt(pred,img0,xyz,R=25,azi=90,logger=None):\n",
    "    img_height = img0.shape[1]\n",
    "    img_width = img0.shape[2]\n",
    "    \n",
    "    scale_y = xyz.shape[0]/img_height\n",
    "    scale_x = xyz.shape[1]/img_width \n",
    "    obj_dim = [] # center_x, center_y, center_z, width, height, depth, rotation_x, rotation_y, rotation_z\n",
    "    heights = []\n",
    "    for det in pred[0]:\n",
    "        xyxy = det[:4].cpu().numpy()\n",
    "        x0,y0,x1,y1 = xyxy\n",
    "        x0_scaled,y0_scaled,x1_scaled,y1_scaled = x0*scale_x,y0*scale_y,x1*scale_x,y1*scale_y\n",
    "        pol_c_x,pol_c_y = (x0+x1)/2,(y0+y1)/2\n",
    "        ix, iy = int(pol_c_x), int(pol_c_y)\n",
    "        low_y = iy-10 if iy-10>=0 else 0\n",
    "        high_y = iy+10 if iy+10<img_height else img_height\n",
    "        low_x = ix-10 if ix-10>=0 else 0\n",
    "        high_x = ix+10 if ix+10<img_width else img_width\n",
    "        poi = np.unravel_index(img0[2,low_y:high_y,low_x:high_x],img0.shape)\n",
    "        center_x, center_y, center_z = xyz[poi,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projec_2D_pred(pred,img0,scan,R=25,azi=90,logger=None):\n",
    "    scale_y = scan.h/img0.shape[1]\n",
    "    scale_x = scan.w/img0.shape[2]\n",
    "    img_height = img0.shape[1]\n",
    "    img_width = img0.shape[2]\n",
    "    print(f\"Image height: {img_height}\")\n",
    "    print(f\"Image width: {img_width}\")\n",
    "    to_rad = np.pi/180\n",
    "    obj_dim = [] # center_x, center_y, center_z, width, height, depth, rotation_x, rotation_y, rotation_z\n",
    "    heights = [] \n",
    "    for det in pred[0]:\n",
    "        xyxy = det[:4].cpu().numpy()\n",
    "        x0,y0,x1,y1 = xyxy\n",
    "\n",
    "        pol_c_x,pol_c_y = (x0+x1)/2,(y0+y1)/2\n",
    "        width = x1-x0\n",
    "        height = y1-y0\n",
    "\n",
    "        ix, iy = int(pol_c_x), int(pol_c_y)\n",
    "\n",
    "        x0_scaled,y0_scaled,x1_scaled,y1_scaled = x0*scale_x,y0*scale_y,x1*scale_x,y1*scale_y\n",
    "        #print(f\"x0: {x0_scaled},y0: {y0_scaled},x1: {x1_scaled},y1: {y1_scaled}\")\n",
    "        low_y = iy-10 if iy-10>=0 else 0\n",
    "        high_y = iy+10 if iy+10<img_height else img_height\n",
    "        low_x = ix-10 if ix-10>=0 else 0\n",
    "        high_x = ix+10 if ix+10<img_width else img_width\n",
    "        print(f\"low_y: {low_y},high_y: {high_y},low_x: {low_x},high_x: {high_x}\")\n",
    "        r = np.median(img0[2,low_y:high_y,low_x:high_x])\n",
    "        #print(f\"r: {r*R}\")\n",
    "        theta = abs((y0_scaled+y1_scaled)/2 - scan.h/2)\n",
    "        vert_dist = r*R*np.cos(theta*to_rad)\n",
    "        print(f\"pol_c_x: {pol_c_x}\")\n",
    "        angles = 2*np.pi*(pol_c_x/img_width)\n",
    "        print(\"angles: \",angles)\n",
    "        center_x = np.cos(angles)*vert_dist\n",
    "        center_y = np.sin(angles)*vert_dist\n",
    "\n",
    "        #print(f\"theta: {theta}\")\n",
    "        l = 2*np.tan(azi/2*to_rad)*vert_dist\n",
    "        #print(f\"l: {l}\")\n",
    "        det_h = l*(y1_scaled-y0_scaled)/scan.h\n",
    "        center_z = det_h/2\n",
    "        rot = angles\n",
    "        obj_dim.append([center_x,center_y,center_z,width,height,det_h,0,0,rot])\n",
    "        #print(f\"Det_h: {det_h}\")\n",
    "        heights.append([det_h,r*R])\n",
    "    plt.scatter(np.array(obj_dim)[:,0],np.array(obj_dim)[:,1])\n",
    "    plt.scatter(0,0,c='r')\n",
    "    plt.show()\n",
    "    print(obj_dim)\n",
    "    return heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projec_2D_pred(pred,img0[0],s,R=25,azi=90)\n",
    "# det = pred[0]\n",
    "    # xyxy = det[:,:4].cpu().numpy()\n",
    "    # x0,y0,x1,y1 = xyxy\n",
    "    # pol_c_x,pol_c_y = (x0+x1)/2,(y0+y1)/2\n",
    "    # widths = x1-x0\n",
    "    # heights = y1-y0\n",
    "\n",
    "    # ix, iy = pol_c_x.astype(np.int8), pol_c_y.astype(np.int8)\n",
    "    # x0_scaled, y0_scaled, x1_scaled, y1_scaled = x0*scale_x,y0*scale_y,x1*scale_x,y1*scale_y\n",
    "    # low_y = np.where(iy-10>=0,iy-10,0)\n",
    "    # high_y = np.where(iy+10<img_height,iy+10,img_height)\n",
    "    # low_x = np.where(ix-10>=0,ix-10,0)\n",
    "    # high_x = np.where(ix+10<img_width,ix+10,img_width)\n",
    "    # r = np.array([np.median(img0[2,ly:hy,lx:hx].cpu().numpy()) for ly,hy,lx,hx in zip(low_y,high_y,low_x,high_x)])\n",
    "    # print(r)\n",
    "    # theta = (y0_scaled+y1_scaled)/2 - scan.h/2\n",
    "    # #vert_dist = [r_*R*np.cos(theta_*to_rad) for r_,theta_ in zip(r,theta)]\n",
    "    # vert_dist = np.multiply(theta,r)\n",
    "    # angles = 2*np.pi*(pol_c_x/img_width)\n",
    "    # center_x = np.multiply(np.cos(angles),vert_dist)\n",
    "    # center_y = np.multiply(np.sin(angles),vert_dist)\n",
    "    # l = 2*np.tan(azi/2*to_rad)*vert_dist\n",
    "    # det_h = np.multiply(l,(y1_scaled-y0_scaled)/scan.h)\n",
    "    # center_z = det_h/2\n",
    "    # print(f\"x0: {x0},y0: {y0},x1: {x1},y1: {y1}\")\n",
    "    # return np.array([center_x,center_y,center_z,widths,heights,det_h,np.zeros_like(center_x),np.zeros_like(center_x),angles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit proj_vec(pred,img[0],s)\n",
    "#res,h = proj_vec(pred,img[0],s)\n",
    "#plt.scatter(res[:,0],res[:,1])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_vec(pred,img0,scan,R=25,azi=90,logger=None):\n",
    "    scale_y = scan.h/img0.shape[1]\n",
    "    scale_x = scan.w/img0.shape[2]# det = pred[0]\n",
    "    \n",
    "    print(f\"Image height: {img_height}\")\n",
    "    print(f\"Image width: {img_width}\")\n",
    "    to_rad = np.pi/180\n",
    "    obj_dim = [] # center_x, center_y, center_z, width, height, depth, rotation_x, rotation_y, rotation_z\n",
    "    heights = []\n",
    "    \n",
    "    \n",
    "    for det in pred[0]:\n",
    "        xyxy = det[:4].cpu().numpy()\n",
    "        x0,y0,x1,y1 = xyxy\n",
    "\n",
    "        pol_c_x,pol_c_y = (x0+x1)/2,(y0+y1)/2\n",
    "        width = x1-x0\n",
    "        height = y1-y0\n",
    "\n",
    "        ix, iy = int(pol_c_x), int(pol_c_y)\n",
    "\n",
    "        x0_scaled,y0_scaled,x1_scaled,y1_scaled = x0*scale_x,y0*scale_y,x1*scale_x,y1*scale_y\n",
    "        #print(f\"x0: {x0_scaled},y0: {y0_scaled},x1: {x1_scaled},y1: {y1_scaled}\")\n",
    "        low_y = iy-10 if iy-10>=0 else 0\n",
    "        high_y = iy+10 if iy+10<img_height else img_height\n",
    "        low_x = ix-10 if ix-10>=0 else 0\n",
    "        high_x = ix+10 if ix+10<img_width else img_width\n",
    "        print(f\"low_y: {low_y},high_y: {high_y},low_x: {low_x},high_x: {high_x}\")\n",
    "        r = np.median(img0[2,low_y:high_y,low_x:high_x].cpu().numpy())\n",
    "        #print(f\"r: {r*R}\")\n",
    "        theta = abs((y0_scaled+y1_scaled)/2 - scan.h/2)\n",
    "        vert_dist = r*R*np.cos(theta*to_rad)\n",
    "        print(f\"pol_c_x: {pol_c_x}\")\n",
    "        angles = 2*np.pi*(pol_c_x/img_width)\n",
    "        print(\"angles: \",angles)\n",
    "        center_x = np.cos(angles)*vert_dist\n",
    "        center_y = np.sin(angles)*vert_dist\n",
    "\n",
    "        #print(f\"theta: {theta}\")\n",
    "        l = 2*np.tan(azi/2*to_rad)*vert_dist\n",
    "        #print(f\"l: {l}\")\n",
    "        det_h = l*(y1_scaled-y0_scaled)/scan.h\n",
    "        center_z = det_h/2\n",
    "        rot = angles\n",
    "        obj_dim.append([center_x,center_y,center_z,width,0,det_h,0,0,rot])\n",
    "        #print(f\"Det_h: {det_h}\")\n",
    "        heights.append([det_h,r*R])\n",
    "    \n",
    "    pred_dict = {\"pred_boxes\": np.array(obj_dim), \"pred_scores\": pred[:,4], \"pred_labels\": pred[:,5]}\n",
    "    return pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreloaddef projec_2D_pred(pred,img0,scan,R=25,azi=9\n",
    "ROOT = FILE\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT)) # Add ROOT\n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd())) # Relative Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nNuScenes uses the following labels:\\n    CLASS_NAMES: [\\'car\\',\\'truck\\', \\'construction_vehicle\\', \\'bus\\', \\'trailer\\',\\n              \\'barrier\\', \\'motorcycle\\', \\'bicycle\\', \\'pedestrian\\', \\'traffic_cone\\']\\n    Note: \\'pedestrian is predicted as index 9\\'\\n    This program uses has been tested with the Ouster OS0-64 sensor.\\n    Example Input:\\n        python3 live_predictions.py --cfg_file \\'cfgs/nuscenes_models/cbgs_voxel0075_res3d_centerpoint.yaml\\' --ckpt \"../checkpoints/cbgs_voxel0075_centerpoint_nds_6648.pth\" --OU_ip \"192.168.200.78\" --TD_ip \"192.168.200.103\" --TD_port 7002  --time 300 --udp_port 7001 --tcp_port 7003 --name \"OS0-64\" --visualize\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import matplotlib.pyplot as plt \n",
    "from queue import Queue\n",
    "from copy import copy\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "FILE = os.path.abspath('')\n",
    "ROOT = FILE\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.append(str(ROOT)) # Add ROOT\n",
    "ROOT = Path(os.path.relpath(ROOT, Path.cwd())) # Relative Path\n",
    "import open3d\n",
    "sys.path.insert(0, '../OusterTesting')\n",
    "sys.path.insert(1, '../OpenPCDet-linux')\n",
    "from models.common import DetectMultiBackend\n",
    "import torch.backends.cudnn as cudnn\n",
    "import utils_ouster\n",
    "from tools.transmitter import Transmitter\n",
    "from tools.visual_utils.open3d_live_vis import LiveVisualizer\n",
    "from ouster import client\n",
    "from contextlib import closing\n",
    "from tools.visual_utils import open3d_vis_utils as V\n",
    "#from pcdet.config import cfg, cfg_from_yaml_file\n",
    "#from pcdet.datasets import DatasetTemplate\n",
    "#from pcdet.models import build_network, load_data_to_gpu\n",
    "from tools.xr_synth_utils import CSVRecorder,TimeLogger,filter_predictions,format_predictions,display_predictions\n",
    "from tools.xr_synth_utils import create_logger\n",
    "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr, cv2,\n",
    "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
    "from utils.plots import Annotator, colors, save_one_box\n",
    "from utils.torch_utils import select_device, time_sync\n",
    "from utils.augmentations import Albumentations, augment_hsv, copy_paste, letterbox, mixup, random_perspective\n",
    "class live_stream:\n",
    "    \"\"\"\n",
    "    Class to stream data from a Sensor.\n",
    "    Inheritance:\n",
    "        DatasetTemplate:\n",
    "            Uses batch processing and collation.\n",
    "    \"\"\"\n",
    "    def __init__(self, classes,ip,stride=32,img_size=(1280,640), logger=None,auto=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_path:\n",
    "            dataset_cfg:\n",
    "            class_names:\n",
    "            training:\n",
    "            logger:\n",
    "        \"\"\"\n",
    "        self.classes = classes\n",
    "        self.ip = ip\n",
    "\n",
    "        self.stride = stride\n",
    "        self.img_size = img_size\n",
    "        self.logger = logger\n",
    "        self.auto = auto\n",
    "        self.rect = True\n",
    "        self.frame = 0\n",
    "        \n",
    "    def prep(self,img0):\n",
    "        \"\"\"started\n",
    "        Prepare data from the lidar sensor.\n",
    "        args:\n",
    "            points: xyz/xyzr points from sensor.\n",
    "        \"\"\"\n",
    "        #print(self.img_size)\n",
    "        #print(img0.shape)\n",
    "        img = self.reshape(copy(img0))\n",
    "       # print(img.shape)\n",
    "        if len(img.shape) == 3:\n",
    "            img = img[None]\n",
    "        #img = img[..., ::-1].transpose((0,3,1,2))  # BGR to RGB, BHWC to BCHW\n",
    "        img = img.transpose((0,3,1,2))  # BGR to RGB, BHWC to BCHW\n",
    "        \n",
    "        img = np.ascontiguousarray(img)\n",
    "        self.frame += 1\n",
    "        return img0,img\n",
    "    def reshape(self,img):\n",
    "        \"\"\"\n",
    "        Reshape the data to be compatible with the model.\n",
    "        Args:\n",
    "            img:\n",
    "        \"\"\"\n",
    "        img = cv2.resize(img,self.img_size,interpolation=cv2.INTER_LINEAR)\n",
    "        return img\n",
    "\n",
    "def initialize_network(args,device):\n",
    "    device = select_device(args.device)\n",
    "    model = DetectMultiBackend(args.weights, device=device, dnn=args.dnn, data=args.data, fp16=args.half)\n",
    "    stride, names, pt = model.stride, model.names, model.pt\n",
    "    imgsz = (args.imgsz, args.imgsz) if isinstance(args.imgsz, int) else args.imgsz  # tuple\n",
    "    imgsz = check_img_size(imgsz=imgsz, s=stride)\n",
    "    imgsz = (imgsz[0],imgsz[0])\n",
    "    model.warmup(imgsz=(1 if pt else 1, 3, *imgsz))\n",
    "    live = live_stream(classes=names,ip=args.OU_ip,stride=stride,auto=args.auto)\n",
    "\n",
    "    return model, stride, names, pt, device,live\n",
    "def initialize_timer(transmitter,logger,args):\n",
    "    time_logger = TimeLogger(logger,args.disp_pred)\n",
    "    time_logger.create_metric(\"Ouster Processing\")\n",
    "    #time_logger.create_metric(\"Pre Processing\")\n",
    "    #time_logger.create_metric(\"Load GPU\")\n",
    "    time_logger.create_metric(\"Infrence\")\n",
    "    time_logger.create_metric(\"Post Processing\")\n",
    "    time_logger.create_metric(\"Format Predictions\")\n",
    "\n",
    "    if args.visualize:\n",
    "        time_logger.create_metric(\"Visualize\")\n",
    "    if args.save_csv:\n",
    "        time_logger.create_metric(\"Save CSV\")\n",
    "    if transmitter.started_udp:\n",
    "        time_logger.create_metric(\"Transmit TD\")\n",
    "    if transmitter.started_ml:\n",
    "        time_logger.create_metric(\"Transmit UE5\")\n",
    "    time_logger.create_metric(\"Full Pipeline\")\n",
    "\n",
    "    return time_logger\n",
    "def visualize_yolo(pred,img,args,fig=None,plot=None,names=None,logger=None):\n",
    "    detections = 0\n",
    "    #print(f\"Pre viz Average img: {img.mean()}\")\n",
    "    for i,det in enumerate(pred):\n",
    "        detections += 1\n",
    "        annotator = Annotator(img.cpu().numpy()[0], line_width=args.line_thickness, example=str(names))\n",
    "        if len(det):\n",
    "            #print(img.shape[2:],img.squeeze().permute(1,2,0).shape)\n",
    "            det[:,:4] = scale_coords(img.shape[2:], det[:,:4], img.squeeze().permute(1,2,0).shape).round()\n",
    "            # if args.disp_pred and logger is not None:\n",
    "            #     for c in det[:, -1].unique():\n",
    "            #         n = (det[:, -1] == c).sum()  # detections per class\n",
    "            #         s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "            # logger.info(f\"{names[int(c)]} detections: {s}\")\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                c = int(cls)  # integer class\n",
    "                label = None if args.hide_labels else (names[c] if args.hide_conf else f'{names[c]} {conf:.2f}')\n",
    "                annotator.box_label(xyxy, label, color=(255,255,255))#colors(c, True))\n",
    "            img0 = annotator.result()\n",
    "            print(img0.shape)\n",
    "            logger.info(f\"Det: {det}\")\n",
    "            cv2.imshow(\"Predictions\",cv2.cvtColor(img0.transpose(1,2,0),cv2.COLOR_RGB2BGR))\n",
    "            cv2.waitKey(1)\n",
    "            time.sleep(100)\n",
    "            exit()\n",
    "        else:\n",
    "            #print(img.shape)\n",
    "            cv2.imshow(\"Predictions\",cv2.cvtColor(img[0].cpu().numpy().transpose(1,2,0),cv2.COLOR_RGB2BGR))\n",
    "            #print(f\"Post viz Average img: {img.mean()}\")\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_config():\n",
    "    parser = argparse.ArgumentParser(description='arg parser')\n",
    "    #parser.add_argument('--cfg_file', type=str, default='cfgs/kitti_models/second.yaml',\n",
    "    #                    help='specify the config for demo')\n",
    "    parser.add_argument('--weights', nargs='+', type=str, default=ROOT / 'yolov5s.pt', help='model path(s)')\n",
    "    parser.add_argument('--imgsz', '--img', '--img-size', nargs='+', type=int, default=1280, help='inference size h,w')\n",
    "    parser.add_argument('--data', type=str, default=ROOT / 'data/coco128.yaml', help='(optional) dataset.yaml path')\n",
    "    parser.add_argument('--max_det', type=int, default=1000, help='maximum detections per image')\n",
    "    parser.add_argument('--conf_thres', type=float, default=0.25, help='confidence threshold')\n",
    "    parser.add_argument('--iou_thres', type=float, default=0.45, help='NMS IoU threshold')\n",
    "    parser.add_argument('--line_thickness', default=3, type=int, help='bounding box thickness (pixels)')\n",
    "    parser.add_argument('--hide_labels', default=False, action='store_true', help='hide labels')\n",
    "    parser.add_argument('--hide_conf', default=False, action='store_true', help='hide confidences')\n",
    "    parser.add_argument('--half', action='store_true', help='use FP16 half-precision inference')\n",
    "    parser.add_argument('--dnn', action='store_true', help='use OpenCV DNN for ONNX inference')\n",
    "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')    \n",
    "    parser.add_argument('--ckpt', type=str, default=None, help='specify the pretrained model')\n",
    "    parser.add_argument('--auto', action='store_true', help='auto size using the model')\n",
    "    parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
    "    parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --classes 0, or --classes 0 2 3')\n",
    "\n",
    "    #parser.add_argument('--ext', type=str, default='.bin', help='specify the extension of your point cloud data file')\n",
    "    parser.add_argument('--OU_ip', type=str, default=None, help='specify the ip of the sensor')\n",
    "    parser.add_argument('--name', type=str, default=None, help='specify the name of the sensor')\n",
    "    parser.add_argument('--UE5_ip', type=str, default=None, help='specify the ip of the UE5 machine')\n",
    "    parser.add_argument('--TD_ip', type=str, default=None, help='specify the ip of the TD machine')\n",
    "\n",
    "    parser.add_argument('--udp_port', type=int, default=7502, help='specify the udp port of the sensor')\n",
    "    parser.add_argument('--tcp_port', type=int, default=7503, help='specify the tcp port of the sensor')\n",
    "    parser.add_argument('--TD_port', type=int, default=7002, help='specify the port of the TD machine')\n",
    "    parser.add_argument('--UE5_port', type=int, default=7000, help='specify the port of the UE5 machine')\n",
    "    parser.add_argument('--time', type=int, default=100\n",
    "    , help='specify the time to stream data from a sensor')\n",
    "    #parser.add_argument('--save_dir', type=str, default=\"../lidarCSV\", help='specify the save directory')\n",
    "    #parser.add_argument('--save_name', type=str, default=\"test_csv\", help='specify the save name')\n",
    "    if sys.version_info >= (3,9):\n",
    "        parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
    "        parser.add_argument('--visualize', action=argparse.BooleanOptionalAction)\n",
    "        parser.add_argument('--save_csv', action=argparse.BooleanOptionalAction)\n",
    "        parser.add_argument('--log_time', action=argparse.BooleanOptionalAction)\n",
    "        parser.add_argument('--disp_pred', action=argparse.BooleanOptionalAction) \n",
    "        \n",
    "\n",
    "    else:\n",
    "        parser.add_argument('--visualize', action='store_true')\n",
    "        parser.add_argument('--no-visualize', dest='visualize', action='store_false')\n",
    "        parser.add_argument('--save_csv', action='store_true')\n",
    "        parser.add_argument('--no-save_csv', dest='save_csv', action='store_false')\n",
    "        parser.add_argument('--log_time', action='store_true')\n",
    "        parser.add_argument('--no-log_time', dest='log_time', action='store_false')\n",
    "        parser.add_argument('--disp_pred', action='store_true')\n",
    "        parser.add_argument('--no-disp_pred', dest='disp_pred', action='store_false')\n",
    "        parser.set_defaults(visualize=True)\n",
    "        parser.set_defaults(save_csv=False)\n",
    "    args = parser.parse_args()\n",
    "    with open(args.data,'r') as f:\n",
    "        try:\n",
    "            data_config = yaml.safe_load(f)\n",
    "        except:\n",
    "            raise ValueError(f\"Invalid data config file: {args.data}\")\n",
    "        #cfg_from_yaml_file(args.cfg_file, cfg)\n",
    "\n",
    "    return args,data_config#, cfg\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    args,data_config = parse_config()\n",
    "    fig = None\n",
    "    plot = None\n",
    "    cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "    #model = DetectMultiBackend(args.weights, device=device, dnn=args.dnn, data=args.data, fp16=args.half)\n",
    "    logger = create_logger()\n",
    "    model, stride, names, pt, device,live = initialize_network(args,device)\n",
    "    if args.OU_ip is None and args.name is None:\n",
    "        raise ValueError('Please specify the ip or sensor name of the ')\n",
    "    # Select classes to use, None -> all.\n",
    "    # classes_to_use = [8]\n",
    "    # Set up interactions\n",
    "    #live = live_stream(cfg.DATA_CONFIG, cfg.CLASS_NAMES, logger=logger)\n",
    "    if args.save_csv:\n",
    "        recorder = CSVRecorder(args.save_name,args.save_dir, cfg.CLASS_NAMES)\n",
    "    limits = {\"ir\":6000,\"reflectivity\": 255, \"range\":25000}\n",
    "    #if range_limit is not None:\n",
    "    #    cfg.DATA_CONFIG.POINT_CLOUD_RANGE = [-range_limit[0],-range_limit[1],-range_limit[2],range_limit[0],range_limit[1],range_limit[2]]\n",
    "        \n",
    "    # Set up network\n",
    "    #model = initialize_network(cfg,args,logger,live)\n",
    "    # Set up local network ports for IO\n",
    "    transmitter = Transmitter(reciever_ip=args.TD_ip, reciever_port=args.TD_port, classes_to_send=[9])\n",
    "    transmitter.start_transmit_udp()\n",
    "    transmitter.start_transmit_ml()\n",
    "    try:\n",
    "        [cfg_ouster, host_ouster] = utils_ouster.sensor_config(args.name if args.name is not None else args.OU_ip,args.udp_port,args.tcp_port)\n",
    "    except:\n",
    "        raise ConnectionError('Could not connect to the sensor')\n",
    "    log_time = False # False to let the program run for one loop to warm up :)\n",
    "    if args.log_time:\n",
    "        time_logger = initialize_timer(logger=logger,transmitter=transmitter,args=args)\n",
    "\n",
    "\n",
    "    with closing(client.Scans.stream(host_ouster, args.udp_port,complete=False)) as stream:\n",
    "        logger.info(f\"Streaming lidar data to: Yolov5 using {args.weights}\")\n",
    "         # time \n",
    "        \n",
    "        start_stream = time.monotonic()\n",
    "        \n",
    "        for i,scan in enumerate(stream): # Ouster scan object\n",
    "            if log_time:\n",
    "                time_logger.start(\"Ouster Processing\")\n",
    "            # Get lidar data\n",
    "            img0 = utils_ouster.ir_ref_range(stream,scan,limits)\n",
    "            \n",
    "            #img0 /= 255\n",
    "            img0, img = live.prep(img0)\n",
    "            if len(img0.shape) == 3:\n",
    "                img0 = img0[None]\n",
    "            img = torch.from_numpy(img).to(device)\n",
    "            img = img.half() if model.fp16 else img.float()  # uint8 to fp16/32\n",
    "            if log_time:\n",
    "                time_logger.stop(\"Ouster Processing\")\n",
    "            \n",
    "            #if range_limit is not None:\n",
    "            #    xyzr = utils_ouster.trim_xyzr(xyzr,range_limit)\n",
    "            #xyzr = utils_ouster.trim_data(data=xyzr,range_limit=range_limit,source=stream,scan=scan)\n",
    "            #print(f\"Input point cloud shape: {xyzr.shape}\")\n",
    "            if i%2 == 0 and log_time:\n",
    "                time_logger.start(\"Full Pipeline\")\n",
    "            if i%2 == 1 and log_time and i != 1:\n",
    "                time_logger.stop(\"Full Pipeline\")\n",
    "            i+=1\n",
    "            #if log_time:\n",
    "            #    time_logger.start(\"Data Prep\")\n",
    "            #data_dict = live.prep(xyzr)\n",
    "            #if log_time:\n",
    "            #    time_logger.stop(\"Data Prep\")\n",
    "            #print(f\"data_dict: {data_dict}\\npoints.shape = {data_dict['points'].shape}\")\n",
    "            #print(f\"points: {sum(data_dict['points'][:,0]==0)}\")\n",
    "            #print(f\"points: {(data_dict['points'][:8,:])}\")\n",
    "\n",
    "            #print(f\"points.shape: {data_dict['points'][0].shape}\")\n",
    "            \n",
    "\n",
    "            #if log_time:\n",
    "            #    time_logger.start(\"Load GPU\")\n",
    "            #load_data_to_gpu(data_dict)\n",
    "            #if log_time:\n",
    "            #    time_logger.stop(\"Load GPU\")\n",
    "           # print(img.shape)\n",
    "            if log_time:\n",
    "                time_logger.start(\"Infrence\")\n",
    "            \n",
    "            pred = model(img,augment=args.augment)\n",
    "            if log_time:\n",
    "                time_logger.stop(\"Infrence\")\n",
    "            if log_time:\n",
    "                time_logger.start(\"Post Processing\")\n",
    "            pred = non_max_suppression(pred, args.conf_thres, args.iou_thres, args.classes, args.agnostic_nms, max_det=args.max_det)\n",
    "            if log_time:\n",
    "                time_logger.stop(\"Post Processing\")\n",
    "            #print(pred)\n",
    "            \n",
    "            #if log_time:\n",
    "            #    time_logger.start(\"Filter Predictions\")\n",
    "            # Only uses pred_dicts[0] since batch size is one at live infrence\n",
    "            #pred_dicts = filter_predictions(pred_dicts[0], classes_to_use)\n",
    "            #if log_time:\n",
    "            #    time_logger.stop(\"Filter Predictions\")\n",
    "                \n",
    "            \n",
    "            \n",
    "            # if args.save_csv: # If recording, save to csv\n",
    "            #     if log_time:\n",
    "            #         time_logger.start(\"Save CSV\")\n",
    "            #     recorder.add_frame_file(copy(data_dict[\"points\"][:,1:-1]).cpu().numpy(),pred_dicts)\n",
    "            #     if log_time:\n",
    "            #         time_logger.stop(\"Save CSV\")\n",
    "                #logger.info(f\"Time to save to csv: {time.monotonic() - start:.3e}\")\n",
    "            \n",
    "            # if transmitter.started_ml:\n",
    "            #     if log_time:\n",
    "            #         time_logger.start(\"Transmit UE5\")\n",
    "            #     transmitter.pcd = copy(data_dict[\"points\"][:,1:])\n",
    "            #     transmitter.pred_dict = copy(pred_dicts)\n",
    "            #     transmitter.send_pcd()\n",
    "            #     if log_time:\n",
    "            #         time_logger.stop(\"Transmit UE5\")\n",
    "\n",
    "\n",
    "            # if transmitter.started_udp: # If transmitting, send to udp\n",
    "            #     if log_time:\n",
    "            #         time_logger.start(\"Transmit TD\")\n",
    "            #     transmitter.pred_dict = copy(pred_dicts)\n",
    "            #     transmitter.send_dict()\n",
    "            #     if log_time:\n",
    "            #         time_logger.stop(\"Transmit TD\")\n",
    "\n",
    "            #logger.info(f\"Frame {live.frame}\")\n",
    "            if args.visualize:\n",
    "\n",
    "                if log_time:\n",
    "                    time_logger.start(\"Visualize\")\n",
    "                visualize_yolo(pred,img,args,names = data_config[\"names\"],logger=logger)\n",
    "                if log_time:\n",
    "                    time_logger.stop(\"Visualize\")\n",
    "                #vis = V.create_live_scene(data_dict['points'][:,1:],ref_boxes=pred_dicts[0]['pred_boxes'],\n",
    "                #ref_scores=pred_dicts[0]['pred_scores'], ref_labels=pred_dicts[0]['pred_labels'])\n",
    "            # #elif args.visualize:\n",
    "            #     start = time.monotonic()\n",
    "            #     #V.update_live_scene(vis,pts,points=data_dict['points'][:,1:], ref_boxes=pred_dicts[0]['pred_boxes'],\n",
    "            #     #    ref_scores=pred_dicts[0]['pred_scores'], ref_labels=pred_dicts[0]['pred_labels'],class_names=cfg.CLASS_NAMES)\n",
    "            #     if log_time:\n",
    "            #         time_logger.start(\"Visualize\")\n",
    "            #     vis.update(points=data_dict['points'][:,1:], \n",
    "            #                 pred_boxes=pred_dicts['pred_boxes'],\n",
    "            #                 pred_labels=pred_dicts['pred_labels'],\n",
    "            #                 pred_scores=pred_dicts['pred_scores'],\n",
    "            #                 )\n",
    "            #     if log_time:\n",
    "            #         time_logger.stop(\"Visualize\")\n",
    "            if time.monotonic()-start_stream > args.time:\n",
    "                stream.close()\n",
    "                break\n",
    "            if log_time and args.disp_pred:\n",
    "                print(\"\\n\")\n",
    "            #if i == 6:\n",
    "            #    break \n",
    "            log_time = args.log_time\n",
    "    transmitter.stop_transmit_udp()\n",
    "    transmitter.stop_transmit_ml()\n",
    "    if log_time:\n",
    "        time_logger.visualize_results()\n",
    "    logger.info(\"Stream Done\")\n",
    "\n",
    "\"\"\"\n",
    "NuScenes uses the following labels:\n",
    "    CLASS_NAMES: ['car','truck', 'construction_vehicle', 'bus', 'trailer',\n",
    "              'barrier', 'motorcycle', 'bicycle', 'pedestrian', 'traffic_cone']\n",
    "    Note: 'pedestrian is predicted as index 9'\n",
    "    This program uses has been tested with the Ouster OS0-64 sensor.\n",
    "    Example Input:\n",
    "        python3 live_predictions.py --cfg_file 'cfgs/nuscenes_models/cbgs_voxel0075_res3d_centerpoint.yaml' --ckpt \"../checkpoints/cbgs_voxel0075_centerpoint_nds_6648.pth\" --OU_ip \"192.168.200.78\" --TD_ip \"192.168.200.103\" --TD_port 7002  --time 300 --udp_port 7001 --tcp_port 7003 --name \"OS0-64\" --visualize\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ab7ca3e99322de2ac7336384dab55ac042d6b7cbdfb1c65d8b85c2db9c4aaac0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
