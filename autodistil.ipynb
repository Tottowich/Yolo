{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoDistil\n",
    "This notebook covers the use of [Roboflow's autodistillation](https://blog.roboflow.com/autodistill/) feature.\n",
    "## Overview\n",
    "Model distillation is a technique for compressing a large model into a smaller model. The smaller model is trained to mimic the behavior of the larger model. This is useful for deploying models to devices with limited memory and processing power, such as mobile phones and embedded devices. What *Autodistill* aims to do is to automate the processes of training a computer vision model using initially unlabeled data. Instead autodistill uses a large model trained on a large dataset to label the unlabeled data. The labeled data is then used to train a smaller model which can be deployed to edge devices.\n",
    "## Setup\n",
    "### Install Dependencies\n",
    "This notebook utilizes the pre-trained [SAM-Segment Anything Model](https://segment-anything.com), [[Github]](https://github.com/facebookresearch/segment-anything) [[Paper]](https://ai.facebook.com/research/publications/segment-anything/), and \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schenk_dec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
